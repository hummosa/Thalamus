{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ade097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# system\n",
    "import os\n",
    "import sys\n",
    "root = os.getcwd()\n",
    "sys.path.append(root)\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "import json\n",
    "# tools\n",
    "import time\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "# computation\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "# tasks\n",
    "import gym\n",
    "import neurogym as ngym\n",
    "from neurogym.wrappers import ScheduleEnvs\n",
    "from neurogym.utils.scheduler import RandomSchedule\n",
    "# models\n",
    "# from model_dev import RNN_MD\n",
    "# from model_dev import serial_RNN_MD as RNN_MD\n",
    "# from utils import get_full_performance\n",
    "# visualization\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = True\n",
    "# mpl.rcParams['pdf.fonttype'] = 42\n",
    "# mpl.rcParams['ps.fonttype'] = 42\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "import argparse\n",
    "my_parser = argparse.ArgumentParser(description='Train neurogym tasks sequentially')\n",
    "my_parser.add_argument('exp_name',\n",
    "                       default='unamed_exps',\n",
    "                       type=str, nargs='?',\n",
    "                       help='Experiment name, also used to create the path to save results')\n",
    "my_parser.add_argument('use_gates',\n",
    "                       default=0, nargs='?',\n",
    "                       type=int,\n",
    "                       help='Use multiplicative gating or not')\n",
    "my_parser.add_argument('same_rnn',\n",
    "                       default=1, nargs='?',\n",
    "                       type=int,\n",
    "                       help='Train the same RNN for all task or create a separate RNN for each task')\n",
    "my_parser.add_argument('train_to_criterion',\n",
    "                       default=1, nargs='?',\n",
    "                       type=int,\n",
    "                       help='TODO')\n",
    "my_parser.add_argument('--var1',\n",
    "                       default=1.0, nargs='?',\n",
    "                       type=float,\n",
    "                       help='the variance of the fixed multiplicative MD to RNN weights')\n",
    "my_parser.add_argument('--var2',\n",
    "                       default=1.0, nargs='?',\n",
    "                       type=float,\n",
    "                       help='the variance of the fixed multiplicative MD to RNN weights')\n",
    "my_parser.add_argument('--num_of_tasks',\n",
    "                       default=30, nargs='?',\n",
    "                       type=int,\n",
    "                       help='number of tasks to train on')\n",
    "\n",
    "\n",
    "# Execute the parse_args() method\n",
    "args = my_parser.parse_args()\n",
    "\n",
    "exp_name = args.exp_name\n",
    "os.makedirs('./files/'+exp_name, exist_ok=True)\n",
    "\n",
    "###--------------------------Training configs--------------------------###\n",
    "\n",
    "# set device\n",
    "device = 'cuda' # always CPU\n",
    "# Config\n",
    "config = {\n",
    "    # exp:\n",
    "    'exp_name': exp_name,\n",
    "    # envs\n",
    "    'tasks': ['yang19.dlygo-v0',\n",
    "            'yang19.rtgo-v0',\n",
    "            'yang19.dlyanti-v0',\n",
    "            'yang19.go-v0',\n",
    "            'yang19.dms-v0',\n",
    "            'yang19.dnms-v0',\n",
    "            'yang19.dmc-v0',\n",
    "            'yang19.dnmc-v0',\n",
    "            'yang19.dm1-v0',\n",
    "            'yang19.dm2-v0',\n",
    "            'yang19.ctxdm1-v0',\n",
    "            'yang19.ctxdm2-v0',\n",
    "            'yang19.multidm-v0',\n",
    "            'yang19.anti-v0',\n",
    "            'yang19.rtanti-v0'\n",
    "            ],\n",
    "    'env_kwargs': {'dt': 100},\n",
    "    'seq_len': 50,\n",
    "# Training\n",
    "    'trials_per_task' : 200000,\n",
    "    'batch_size' : 100,\n",
    "    'print_every_batches': 100,\n",
    "    'train_to_criterion': bool(args.train_to_criterion),\n",
    "    'device': device,\n",
    "# model\n",
    "    'use_lstm': False,\n",
    "    'same_rnn' : bool(args.same_rnn), \n",
    "    'use_gates': bool(args.use_gates), \n",
    "    'md_mean' : args.var1,\n",
    "    'md_range': args.var2, #0.1\n",
    "    'use_external_inputs_mask': False,\n",
    "    'input_size': 33,\n",
    "    'hidden_size': 256,\n",
    "    'sub_size': 128,\n",
    "    'output_size': 17,\n",
    "    'num_task': 2,\n",
    "    'MDeffect': False,\n",
    "    'md_size': 15,\n",
    "    'md_active_size': 5,\n",
    "    'md_dt': 0.001,\n",
    "# optimizer\n",
    "    'lr': 1e-4, # 1e-4 for CTRNN, 1e-3 for LSTM\n",
    "}\n",
    "config.update({'tasks': config['tasks'][:args.num_of_tasks]})\n",
    "config.update({'human_task_names': ['{:<6}'.format(tn[7:-3]) for tn in config['tasks']]})\n",
    "config.update({'md_size': len(config['tasks'])})\n",
    "\n",
    "exp_signature = config['exp_name'] +f'_{args.var1}_{args.var2}_'+\\\n",
    "    f'{\"same_rnn\" if config[\"same_rnn\"] else \"separate\"}_{\"gates\" if config[\"use_gates\"] else \"nogates\"}'+\\\n",
    "        f'_{\"tc\" if config[\"train_to_criterion\"] else \"nc\"}'\n",
    "print(exp_signature)\n",
    "\n",
    "task_seq = []\n",
    "# Add tasks gradually with rehearsal 1 2 1 2 3 1 2 3 4 ...\n",
    "task_sub_seqs = [[(i, config['tasks'][i]) for i in range(s)] for s in range(2, len(config['tasks']))] # interleave tasks and add one task at a time\n",
    "for sub_seq in task_sub_seqs: task_seq+=sub_seq\n",
    "\n",
    "# Just sequence the tasks serially\n",
    "simplified_task_seq = [(i, config['tasks'][i]) for i in range(len(config['tasks']))]\n",
    "task_seq = simplified_task_seq\n",
    "# print('Task seq to be learned: ', task_seq)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def get_performance(net, envs, context_ids, batch_size=100):\n",
    "    if type(envs) is not type([]):\n",
    "        envs = [envs]\n",
    "\n",
    "    fixation_accuracies = defaultdict()\n",
    "    action_accuracies = defaultdict()\n",
    "    for task_i, (context_id, env) in enumerate(zip(context_ids, envs)):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        inputs, labels = get_trials_batch(env, batch_size)\n",
    "        if config['use_lstm']:\n",
    "            action_pred, _ = net(inputs) # shape [500, 10, 17]\n",
    "        else:\n",
    "            action_pred, _ = net(inputs, sub_id=context_id) # shape [500, 10, 17]\n",
    "        ap = torch.argmax(action_pred, -1) # shape ap [500, 10]\n",
    "\n",
    "        gt = torch.argmax(labels, -1)\n",
    "\n",
    "        fix_lens = torch.sum(gt==0, 0)\n",
    "        act_lens = gt.shape[0] - fix_lens \n",
    "\n",
    "        fixation_accuracy = ((gt==0)==(ap==0)).sum() / np.prod(gt.shape)## get fixation performance. overlap between when gt is to fixate and when model is fixating\n",
    "           ## then divide by number of time steps.\n",
    "        fixation_accuracies[task_i] = fixation_accuracy.detach().cpu().numpy()\n",
    "        action_accuracy = (gt[-1,:] == ap[ -1,:]).sum() / gt.shape[1] # take action as the argmax of the last time step\n",
    "        action_accuracies[task_i] = action_accuracy.detach().cpu().numpy()\n",
    "#         import pdb; pdb.set_trace()\n",
    "    return((fixation_accuracies, action_accuracies))\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def create_log (task_i, task_id, task_name):\n",
    "    return     ({\n",
    "        'task_i': task_i,\n",
    "        'task_id': task_id,\n",
    "        'task_name' : task_name,\n",
    "        'stamps': [],\n",
    "        'losses': [],\n",
    "        'gradients': [],\n",
    "        'accuracy' : [],\n",
    "        'fixation_accuracy': [],\n",
    "    })\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def accuracy_metric(outputs, labels):\n",
    "    ap = torch.argmax(outputs, -1) # shape ap [500, 10]\n",
    "    gt = torch.argmax(labels, -1)\n",
    "    action_accuracy = (gt[-1, :] == ap[-1,:]).sum() / gt.shape[1] # take action as the argmax of the last time step\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return(action_accuracy.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def get_trials_batch(envs, batch_size):\n",
    "    # check if only one env or several and ensure it is a list either way.\n",
    "    if type(envs) is not type([]):\n",
    "        envs = [envs]\n",
    "        \n",
    "    # fetch and batch data\n",
    "    obs, gts = [], []\n",
    "    for bi in range(batch_size):\n",
    "        env = envs[np.random.randint(0, len(envs))] # randomly choose one env to sample from, if more than one env is given\n",
    "        env.new_trial()\n",
    "        ob, gt = env.ob, env.gt\n",
    "        assert not np.any(np.isnan(ob))\n",
    "        obs.append(ob), gts.append(gt)\n",
    "    # Make trials of equal time length:\n",
    "    obs_lens = [len(o) for o in obs]\n",
    "    max_len = np.max(obs_lens)\n",
    "    for o in range(len(obs)):\n",
    "        while len(obs[o]) < max_len:\n",
    "            obs[o]= np.insert(obs[o], 0, obs[o][0], axis=0)\n",
    "#             import pdb; pdb.set_trace()\n",
    "    gts_lens = [len(o) for o in gts]\n",
    "    max_len = np.max(gts_lens)\n",
    "    for o in range(len(gts)):\n",
    "        while len(gts[o]) < max_len:\n",
    "            gts[o]= np.insert(gts[o], 0, gts[o][0], axis=0)\n",
    "\n",
    "\n",
    "    obs = np.stack(obs) # shape (batch_size, 32, 33)\n",
    "    \n",
    "    gts = np.stack(gts) # shape (batch_size, 32)\n",
    "\n",
    "    # numpy -> torch\n",
    "    inputs = torch.from_numpy(obs).type(torch.float).to(device)\n",
    "    labels = torch.from_numpy(gts).type(torch.long).to(device)\n",
    "\n",
    "    # index -> one-hot vector\n",
    "    labels = (F.one_hot(labels, num_classes=config['output_size'])).float() \n",
    "    return (inputs.permute([1,0,2]), labels.permute([1,0,2])) # using time first [time, batch, input]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "class CTRNN_MD(nn.Module):\n",
    "    \"\"\"Continuous-time RNN that can take MD inputs.\n",
    "    Args:\n",
    "        input_size: Number of input neurons\n",
    "        hidden_size: Number of hidden neurons\n",
    "        sub_size: Number of subpopulation neurons\n",
    "    Inputs:\n",
    "        input: (seq_len, batch, input_size), network input\n",
    "        hidden: (batch, hidden_size), initial hidden activity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, dt=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_size =  config['input_size']\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.output_size = config['output_size']\n",
    "        self.num_task = config['num_task']\n",
    "        self.md_size = config['md_size']\n",
    "        self.use_external_input_mask = config['use_external_inputs_mask']\n",
    "        self.use_multiplicative_gates = config['use_gates']\n",
    "        self.device = config['device']\n",
    "\n",
    "        self.tau = 100\n",
    "        if dt is None:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = dt / self.tau\n",
    "        self.alpha = alpha\n",
    "        self.oneminusalpha = 1 - alpha\n",
    "\n",
    "        if self.use_multiplicative_gates:\n",
    "            self.gates = torch.normal(config['md_mean'], 1., size=(config['md_size'], config['hidden_size'], ),\n",
    "              dtype=torch.float) #.type(torch.LongTensor) device=self.device,\n",
    "            # control density \n",
    "            self.gates_mask = np.random.uniform(0, 1, size=(config['md_size'], config['hidden_size'], )) \n",
    "            self.gates_mask = (self.gates_mask < config['md_range']).astype(float)\n",
    "            self.gates = torch.from_numpy(self.gates_mask) #* torch.abs(self.gates) \n",
    "            self.gates = self.gates.to(device).float() #\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # torch.uniform(config['md_mean'], 1., size=(config['md_size'], config['hidden_size'], ),\n",
    "            #  device=self.device, dtype=torch.float)\n",
    "                # *config.G/np.sqrt(config.Nsub*2)\n",
    "            # Substract mean from each row.\n",
    "            # self.gates -= np.mean(self.gates, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # sensory input layer\n",
    "        self.input2h = nn.Linear(self.input_size, self.hidden_size)\n",
    "\n",
    "        # hidden layer\n",
    "        self.h2h = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # identity*0.5\n",
    "        nn.init.eye_(self.h2h.weight)\n",
    "        self.h2h.weight.data *= 0.5\n",
    "\n",
    "    def init_hidden(self, input):\n",
    "        batch_size = input.shape[1]\n",
    "        # as zeros\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        # as uniform noise\n",
    "        # hidden = 1/self.hidden_size*torch.rand(batch_size, self.hidden_size)\n",
    "        return hidden.to(input.device)\n",
    "\n",
    "    def recurrence(self, input, sub_id, hidden):\n",
    "        \"\"\"Recurrence helper.\"\"\"\n",
    "        ext_input = self.input2h(input)\n",
    "        rec_input = self.h2h(hidden)\n",
    "\n",
    "        # mask external inputs\n",
    "        #  turn off mask external input -> Elman/Elman+MD\n",
    "        if self.use_external_input_mask:\n",
    "            ext_input_mask = torch.zeros_like(rec_input)\n",
    "            ext_input_mask[:, sub_id*self.sub_size:(sub_id+1)*self.sub_size] = 1\n",
    "            ext_input = ext_input.mul(ext_input_mask)\n",
    "\n",
    "        if self.use_multiplicative_gates:\n",
    "            #TODO restructure to have sub_id already passed per trial in a batch, and one_hot encoded [batch, md_size]\n",
    "            batch_sub_encoding = F.one_hot(torch.tensor([sub_id]* input.shape[0]), self.md_size).type(torch.float)\n",
    "            gates = torch.matmul(batch_sub_encoding.to(self.device), self.gates)\n",
    "            rec_input = torch.multiply( gates, rec_input)\n",
    "            # import pdb; pdb.set_trace()\n",
    "        pre_activation = ext_input + rec_input\n",
    "        \n",
    "        h_new = torch.relu(hidden * self.oneminusalpha + pre_activation * self.alpha)\n",
    "\n",
    "        return h_new\n",
    "\n",
    "    def forward(self, input, sub_id, hidden=None):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        \n",
    "        num_tsteps = input.size(0)\n",
    "\n",
    "        # init network activities\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input)\n",
    "        \n",
    "        # initialize variables for saving network activities\n",
    "        output = []\n",
    "        for i in range(num_tsteps):\n",
    "            hidden = self.recurrence(input[i], sub_id, hidden)\n",
    "                \n",
    "            # save PFC activities\n",
    "            output.append(hidden)\n",
    "        \n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output, hidden\n",
    "\n",
    "class RNN_MD(nn.Module):\n",
    "    \"\"\"Recurrent network model.\n",
    "    Args:\n",
    "        input_size: int, input size\n",
    "        hidden_size: int, hidden size\n",
    "        sub_size: int, subpopulation size\n",
    "        output_size: int, output size\n",
    "        rnn: str, type of RNN, lstm, rnn, ctrnn, or eirnn\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = CTRNN_MD(config)\n",
    "        self.drop_layer = nn.Dropout(p=0.05)\n",
    "        self.fc = nn.Linear(config['hidden_size'], config['output_size'])\n",
    "\n",
    "    def forward(self, x, sub_id):\n",
    "        rnn_activity, _ = self.rnn(x, sub_id)\n",
    "        rnn_activity = self.drop_layer(rnn_activity)\n",
    "        out = self.fc(rnn_activity)\n",
    "        return out, rnn_activity\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# main loop\n",
    "\n",
    "def create_model():\n",
    "    # model\n",
    "    if config['use_lstm']:\n",
    "        pass\n",
    "    else:\n",
    "        net = RNN_MD(config)\n",
    "    net.to(device)\n",
    "    return(net )\n",
    "\n",
    "if config['same_rnn']:\n",
    "    net = create_model()\n",
    "    \n",
    "envs = []\n",
    "num_tasks = len(config['tasks'])\n",
    "# Make all tasks\n",
    "for task_id in range(num_tasks):\n",
    "    env = gym.make(config['tasks'][task_id], **config['env_kwargs'])\n",
    "    envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_logs = []\n",
    "testing_logs = []\n",
    "bar_tasks = enumerate(tqdm(task_seq))\n",
    "for task_i, (task_id, task_name) in bar_tasks:\n",
    "  \n",
    "    env = envs[task_id]\n",
    "    tqdm.write('learning task:\\t ' + config['human_task_names'][task_id])\n",
    "    \n",
    "    if not config['same_rnn']:\n",
    "        net= create_model()\n",
    "\n",
    "    # criterion & optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    #     print('training parameters:')\n",
    "    training_params = list()\n",
    "    for name, param in net.named_parameters():\n",
    "    #         print(name)\n",
    "        training_params.append(param)\n",
    "    optimizer = torch.optim.Adam(training_params, lr=config['lr'])\n",
    "\n",
    "\n",
    "    # training\n",
    "    training_log = create_log(task_i,task_id, task_name,)\n",
    "    testing_log = create_log(task_i,task_id, task_name,)\n",
    "    \n",
    "    if config['MDeffect']:\n",
    "        net.rnn.md.learn = True\n",
    "        net.rnn.md.sendinputs = True\n",
    "    running_acc = 0\n",
    "    training_bar = trange(config['trials_per_task']//config['batch_size'])\n",
    "    for i in training_bar:\n",
    "        # control training paradigm\n",
    "        context_id = task_id if config['use_gates'] else 0 \n",
    "\n",
    "        # fetch data\n",
    "        inputs, labels = get_trials_batch(envs=env, batch_size = config['batch_size'])\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        if config['use_lstm']:\n",
    "            outputs, rnn_activity = net(inputs)\n",
    "        else:\n",
    "            outputs, rnn_activity = net(inputs, sub_id=context_id)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc  = accuracy_metric(outputs.detach(), labels.detach())\n",
    "\n",
    "        # save loss\n",
    "        training_log['losses'].append(loss.item())\n",
    "        training_log['stamps'].append(i)\n",
    "        training_log['accuracy'].append(acc)\n",
    "        \n",
    "        training_bar.set_description('loss, acc: {:0.4F}, {:0.3F}'.format(loss.item(), acc))\n",
    "#         training_bar.set_description('loss, acc: {:0.3F}, {0.3F}'.format(loss.item(), acc))\n",
    "\n",
    "        # print statistics\n",
    "        if i % config['print_every_batches'] == (config['print_every_batches'] - 1):\n",
    "            ################################ test during training\n",
    "            net.eval()\n",
    "            if config['MDeffect']:\n",
    "                net.rnn.md.learn = False\n",
    "            with torch.no_grad():\n",
    "                testing_log['stamps'].append(i+1)\n",
    "                #   fixation & action performance\n",
    "#                 print('Performance')\n",
    "                num_tasks = len(config['tasks'])\n",
    "                testing_context_ids = list(range(len(envs) )) if config['use_gates'] else [context_id]*len(envs)\n",
    "                fix_perf, act_perf = get_performance(\n",
    "                    net,\n",
    "                    envs,\n",
    "                    context_ids=testing_context_ids,\n",
    "                    ) \n",
    "                \n",
    "                testing_log['fixation_accuracy'].append(fix_perf)\n",
    "                testing_log['accuracy'].append(act_perf)\n",
    "\n",
    "#                 for env_id in range(num_tasks):\n",
    "#                     print('  act performance, task #, name {:d} {}, batch# {:d}: {:0.2f}'.format(\n",
    "#                         env_id+1, config['human_task_names'][env_id], i+1,\n",
    "#                         act_perf[env_id]))\n",
    "            net.train()\n",
    "            if config['MDeffect']:\n",
    "                net.rnn.md.learn = True\n",
    "\n",
    "            if (running_acc > 0.98) and config['train_to_criterion']:\n",
    "                # import pdb; pdb.set_trace()\n",
    "                break # stop training current task if sufficient accuracy. Note placed here to allow at least one performance run before this is triggered.\n",
    "        running_acc = 0.7 * running_acc + 0.3 * acc\n",
    "\n",
    "    training_log['sample_input'] = inputs[0].detach().cpu().numpy().T\n",
    "    training_log['sample_label'] = labels[0].detach().cpu().numpy().T\n",
    "    training_log['sample_output'] = outputs[0].detach().cpu().numpy().T\n",
    "    training_logs.append(training_log)\n",
    "    testing_logs.append(testing_log)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "num_tasks = len(config['tasks'])\n",
    "title_label = 'Training tasks sequentially ---> \\n    ' + config['exp_name']\n",
    "logs = testing_logs\n",
    "\n",
    "max_x = config['trials_per_task']//config['batch_size']\n",
    "fig, axes = plt.subplots(num_tasks,num_tasks, figsize=[9,7])\n",
    "for logi in range(num_tasks):\n",
    "    for li in range(num_tasks):\n",
    "        ax = axes[ li, logi ] # log i goes to the col direction -->\n",
    "        ax.set_ylim([-0.1,1.1])\n",
    "        ax.set_xlim([0, max_x])\n",
    "#         ax.axis('off')\n",
    "        log = testing_logs[logi]\n",
    "        ax.plot(log['stamps'], [test[li] for test in log['accuracy']], linewidth=2)\n",
    "        ax.plot(log['stamps'], np.ones_like(log['stamps'])*0.5, ':', color='grey', linewidth=0.5)\n",
    "        if li == 0: ax.set_title(config['human_task_names'][logi])\n",
    "        if logi == 0: ax.set_ylabel(config['human_task_names'][li])\n",
    "        ax.set_yticklabels([]) \n",
    "        ax.set_xticklabels([])\n",
    "        if logi== li:\n",
    "            ax.axvspan(*ax.get_xlim(), facecolor='grey', alpha=0.2)\n",
    "        if li == num_tasks-1 and logi in [num_tasks//2 - 4, num_tasks//2, num_tasks//2 + 4] :\n",
    "            ax.set_xlabel('batch #')\n",
    "axes[num_tasks-1, num_tasks//2-2].text(-8., -2.5, title_label, fontsize=12)     \n",
    "exp_parameters = f'Exp parameters: {config[\"exp_name\"]}\\nRNN: {\"same\" if config[\"same_rnn\"] else \"separate\"}'+'\\n'+\\\n",
    "      f'mul_gate: {\"True\" if config[\"use_gates\"] else \"False\"}\\\n",
    "          {exp_signature}'\n",
    "axes[num_tasks-1, 0].text(-7., -2.2, exp_parameters, fontsize=7)     \n",
    "# plt.show()\n",
    "plt.savefig('./files/'+ config['exp_name']+f'/testing_log_{exp_signature}.jpg')\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "np.save('./files/'+ config['exp_name']+f'/testing_logs_{exp_signature}.npy', testing_logs, allow_pickle=True)\n",
    "np.save('./files/'+ config['exp_name']+f'/training_logs_{exp_signature}.npy', training_logs, allow_pickle=True)\n",
    "\n",
    "# same_testing_log = np.load('./files/same_model_testing_log.npy', allow_pickle=True)\n",
    "# same_training_log = np.load('./files/same_model_training_log.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# sep_testing_logs = np.load('./files/separate_model_testing_logs.npy', allow_pickle=True)\n",
    "# sep_training_logs = np.load('./files/separate_model_training_logs.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "def show_input_output(input, label, output=None, axes=None):\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(3)\n",
    "                \n",
    "    no_output = True if output is None else False\n",
    "    \n",
    "    axes[0].imshow(input)\n",
    "    axes[1].imshow(label)\n",
    "    if output is not None: axes[2].imshow(output)\n",
    "    \n",
    "    axes[0].set_xlabel('Time steps')\n",
    "#     ax.set_ylabel('fr')\n",
    "#     ax.set_yticks([1, 17, 33, 34, 49])\n",
    "    \n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(num_tasks,4, figsize=[6,14])\n",
    "# for logi in range(num_tasks):\n",
    "#     ax = axes[logi , 0 ]\n",
    "#     ax.set_ylim([-0.1,1])\n",
    "#     ax.axis('off')\n",
    "#     log = training_logs[logi]\n",
    "#     ax.plot(log['stamps'], log['accuracy'])\n",
    "#     show_input_output(log['sample_input'], log['sample_label'], log['sample_output'], axes = axes[logi,1:])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#### draw input output\n",
    "# plt.close('all')\n",
    "# fig, axes = plt.subplots(20,3, figsize=[6,14])\n",
    "# for i in range(20):\n",
    "#     show_input_output(inputs[i].detach().cpu().numpy().T, labels[i].detach().cpu().numpy().T, outputs[i].detach().cpu().numpy().T, axes=axes[i,:])\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
